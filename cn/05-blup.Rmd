# Best Linear Unbiased Prediction (BLUP) {#blup}
The prediction of breeding values using the selection index method always required to correct the information sources for an appropriate comparison value. So far we have referred to that comparison value as the population mean and we have assumed this correction value to be known. In reality, the computation of these comparison values is a difficult problem. This problem is one of the reasons that nowadays the predictions of all breeding values are no longer based on the selection index method, but on a method that is called BLUP. The selection index method is still useful when it comes to predicting the aggregate genotype which will be the topic of a later chapter. In this chapter, we first want to have a closer look at the problem of computing these correction factors with which the information sources must be adjusted with. After that, the BLUP method will be introduced.


## Problem of Correction 
In theory, the population mean is the ideal correction value for all information sources. From our standard model we can derive

\begin{equation}
y = \mu + a + e  \qquad \rightarrow \qquad \bar{y} = \bar{mu} + \bar{a} + \bar{e} = \mu
(\#eq:standardmodelcorrectionvalue)
\end{equation}

Because, we defined the true breeding value $a$ and the non-identifiable environmental effects $e$ as deviations from a common mean, the average effect of all identifiable environmental components is captured by the population mean $\mu$. But this is only true in an idealized population where all selection candidates are kept in the same environment and where they deliver their performances at the same time. In real world scenarios, this is unrealistic, because e.g. own performance values and progeny performances cannot be deliverd at the same time. Futhermore, selection candidates are kept in different herds in different environments. All these factors do have an influence on the performance of the recorded animals and hence on the predicted breeding values. But good methods for predicting breeding values should be able to correct for such environmental influences. If that is not the case, environmental factors will __bias__ the predicted breeding values. To avoid such biases, performance records were subdivided into environmental classes. In dairy cattle such classes were formed based on herds, calving year, calving season and age at first calving. In pigs, performance records might be divided into herds, years and fattening batches. From now on, we call the combination of these environmental effects on the performance records as __identifiable systematic fixed effects__. For the prediction of breeding values, we assume that these fixed effects in a given comparison class have all the same influence on the performance of the animals that are in the same class. Hence if we group all animals who show the same levels of all fixed effects into one comparison class, any biases from the identifiable environment can be avoided. 

The more environmental factors can be considered in forming the comparison classes, the better we can correct our performance records for the environmental effects. But when the number of environmental factors increases the number of animals per comparison class decreases. From the statistical point of view, the small number of observations in comparison classes reduce the accuracy with which the environmental fixed effects can be estimated. With smaller comparison groups, the risk that the average breeding value of animals in such a comparison is not zero increases. In case the average breeding value in a comparison group is not zero, predicted breeding values show a deviation which is called __bias__. The occurence of bias can be shown as follows. Let us assume the average performace of all animals in a comparison group (CG) to be $\bar{y}_{CG}$:

\begin{equation}
\bar{y}_{CG} = \mu + \bar{a}_{CG} + \bar{e}_{CG}
(\#eq:meanphencontemporarygroup)
\end{equation}

In case the average breeding value $\bar{a}_{CG}$ is zero, the population mean $\mu$ measures the average identifiable environment effect. If $\bar{a}_{CG}$ is not zero, then the predicted breeding value $\hat{a}_i$ using the selection index $I$ corresponds to

\begin{align}
I  &=  b(y_i - (\mu + \bar{a}_{CG})) \notag \\
   &=  b(y_i - \mu) - b\bar{a}_{CG} \notag \\
   &=  \hat{a}_i  - b\bar{a}_{CG}
(\#eq:selectionindexbias)   
\end{align}

The second term in the result of \@ref(eq:selectionindexbias) measures the __bias__. This depends on the average breeding values of the animals of the comparison group. If the average breeding value of all animals in the comparison group is zero, then the predicted breeding value from \@ref(eq:selectionindexbias) is unbiased. Because we have to know the breeding values of the animals in the comparison group to get an unbiased prediction of the breeding value for a given animal and the breeding values of the animals in the comparison group must also be predicted, this consists of a "chicken-and-egg" problem which cannot be solved. 

The solution to this was presented by Charles R. Henderson in several publications ([@Henderson1973] and [@Henderson1975]). The key idea behind the solution is to estimate the identifiable environmental factors as fixed effects and to predict the breeding values as random effects simultaneously in a linear mixed effects model. The properties of the methodology developed by Henderson are similar to those of the selection index method. But the main advantage of Henderson's methodologies is that phenotypic records do not need to be corrected before breeding values can be predicted. But the effects of the identifiable environmental factors are also a result which come out of the analysis. The methodology developed by Henderson is called __BLUP__ and the properties of this methodology are directly incorporated into the name where 

* __B__ stands for __best__ which means that the correlation between the true ($a$) and the predicted breeding value ($\hat{a}$) is maximal or the prediction error variance ($var(a - \hat{a})$) is minimal.
* __L__ stands for __linear__ which means the predicted breeding values are linear functions of the observations ($y$)
* __U__ stands for __unbiased__ which means that the expected values of the predicted breeding values are equal to the true breeding values
* __P__ stands for __prediction__ 

BLUP based approaches have found widespread usage in genetic evaluations. They are used for both traditional predictions of breeding values and also for predicting genomic breeding values. The popularity of BLup is not only due to the theoretical foundations behind BLUP, but Henderson has also developed efficient algorithms to be able to compute predicted breeding values for very large livestock breeding populations. The theoretic foundations, the development of efficient algorithms ogether with the availablity of large computational resources at a very low price have made BLUP to become the de-facto standard methodology for predicting breeding values.


## Numeric Example {#blupnumericexample}
We want to use a concrete numeric example of a small population to explain how breeding values are predicted using the BLUP methodology. The phenotypic observations consist of measurements of the trait __weaning weight__ in beef cattle. Table \@ref(tab:TableBeefExample) gives an overview of the dataset.

```{r TableBeefExample, echo=FALSE, results='asis'}
### # fix the numbers parents and offspring
n_nr_sire <- 3
n_nr_dam <- 8
n_nr_parents <- n_nr_sire + n_nr_dam
n_nr_offspring <- 16
n_nr_animals <- n_nr_parents + n_nr_offspring
### # assign parents to offspring and herds to records
vec_sire_id <- c(rep(1,8), rep(2,6), rep(3,2))
vec_dam_id <- rep(4:11,2)
vec_herd_codes <- c(rep(1,4), rep(2,4), rep(1,4), rep(2,4))
### # vector of observations
vec_weaning_weight <-  c(2.61,2.31,2.44,2.41,2.51,2.55,2.14,2.61,2.34,1.99,3.1,2.81,2.14,2.41,2.54,3.16)

### # create a tibble from the data
tbl_beef_data <- dplyr::data_frame( Animal = (n_nr_parents + 1):n_nr_animals,
                                    Sire   = vec_sire_id,
                                    Dam    = vec_dam_id[order(vec_dam_id)],
                                    Herd   = vec_herd_codes,
                                    `Weaning Weight` = vec_weaning_weight )
### # count number of observations
n_nr_observation <- nrow(tbl_beef_data)

### # parameters
h2 <- .25
n_var_p <- round(var(tbl_beef_data$`Weaning Weight`), digits = 4)
n_var_g <- round(h2 * n_var_p, digits = 4)
n_pop_mean <- round(mean(tbl_beef_data$`Weaning Weight`), digits = 2)

### # show the data frame
knitr::kable( tbl_beef_data, 
              format = "latex",
              booktabs = TRUE, 
              longtable = TRUE,
              caption = "Example Data Set for Weaning Weight in Beef Cattle" )
```

We assume the phenotypic variance ($\sigma_p^2$) to be `r n_var_p` and the heritability $(h^2)$ corresponds to `r h2`. 


## Mixed Linear Effects Model {#mixedlineareffectsmodel}
A simple linear model contains fixed effects such as _herd_ or _sex_ of an animal and tries to explain the observations as linear functions of such effects. Because the effects considered in a model cannot account for all influences of a given set of observations, every model must have a random residual component. If a linear model contains besides the residuals any additional random effects, then this model is called a __mixed linear effects model__. 


### Fixed Versus Random Effects {#fixedversusrandomeffects}
Unforunately, there is no unique and generally accepted definition of which effects should be fixed and which should be random. There are generally accepted guidelines of how to classify effects as fixed or as random. Table \@ref(tab:fixedversusrandom) lists a few criteria that might be helpful.

```{r fixedversusrandom, echo=FALSE, results='asis'}
tbl_fixed_versus_random <- tibble::data_frame(`fixed effect` = c("classes can be defined exactly",
                                                                 "the value of a class does not have an apriori expected value",
                                                                 "values are exactly estimable",
                                                                 "the expected value of a class effect is of primary interest",
                                                                 "fixed effects can be corrected for"),
                                              `random effects` = c("realized value come from an underlying distribution",
                                                                   "each realisation is unique",
                                                                   "observations are infuenced by the variance of the random effect",
                                                                   "main interest is on the variance not on the expected value", 
                                                                   ""))
suppressPackageStartupMessages( library(dplyr) ) 
knitr::kable(tbl_fixed_versus_random, 
             format = "latex",
             booktabs = TRUE, 
             longtable = TRUE,
             caption = "Classification Factors of Fixed and Random Effects") %>%
  kableExtra::kable_styling(full_width = F)  %>%
  kableExtra::column_spec(1, width = "22em") %>%
  kableExtra::column_spec(2, width = "22em")
```

Certain factors such as herd, sex, breed or feeding regimes can be classified unambiguously as fixed effects. On the other hand breeding values are always random effects. Because, we know that breeding values have an expected value of $0$ and have a certain variance, they must be modelled as random effects where these properties can be integrated into the model. Furthermore, each animal has a different realisation of a breeding value. Exceptions are mono-clonal twins and clones.

From a practical point of view, the software program that is used to analyse the data has also an influence on whether a certain effect is treated as fixed or as random. If a certain effect has very many levels such as herds, then it is sometimes better for the analysis to treat such an effect as random. 


### Model Specification
In a linear mixed effects model a single observation $y_{ijk}$ is decomposed according to equation \@ref(eq:linearmixedeffectmodelsingleobservation)

\begin{equation}
y_{ijk} = \beta_i + u_j + e_{ijk}
(\#eq:linearmixedeffectmodelsingleobservation)
\end{equation}

where $\beta_i$ stands for the $i-^{th}$ level of a fixed effect, $u_j$ is the $j-{th}$ realisation of the random effect $u$ and $e_{ijk}$  is the residual effect of the $k-^{th}$ observation}.  Because, we do not want to model just one observation, but we want to include all observations of a complete population, it is helpful to convert the model in \@ref(eq:linearmixedeffectmodelsingleobservation) into matrix-vector notation. This is shown in equation \@ref(eq:linearmixedeffectmodelmatrixvector)

\begin{equation}
y = X\beta + Zu + e
(\#eq:linearmixedeffectmodelsingleobservation)
\end{equation}





## Sire Model


## Animal Model
In a first example model for our dataset in Table \@ref(tab:TableBeefExample), we treat the effect of herd $h$ as fixed effect and we include the breeding value of animal $i$ as random effect.



## Appendix: Derivation of BLUP
Consider the mixed linear model

$$y = Xb + Zu + e$$
with $E(y) = Xb$, $E(a) = 0$ and $E(e) = 0$, $var(u) = G$ and $var(e) = R$, hence $var(y) = V = ZGZ^T + R$

Suppose, we want to predict a linear function $k^Tb + a$, using a linear function $L^Ty$ of $y$. Predictor $L^Ty$ is chosen such that 

$$E(L^Ty) = E(k^Tb + a)= E(k^Tb) + E(a) = k^Tb\rightarrow L^TXb = k^Tb \rightarrow k=X^TL$$

and such that the prediction error variance (PEV) is minimal.







